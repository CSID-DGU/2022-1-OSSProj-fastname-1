{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib.request\n",
    "\n",
    "from dateutil.parser import parse\n",
    "from urllib.request import urlopen\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ssl._create_unverified_context()\n",
    "titles = []\n",
    "dates = []\n",
    "links = []\n",
    "# li_tag = []\n",
    "#main > section.community-body > div.community-body__content > div.question-list-container > ul > li:nth-child(20) > a\n",
    "page_num = 1\n",
    "# 24 48 72\n",
    "for page_num in range(1, 3):\n",
    "    url = 'https://www.inflearn.com/community/studies?page='+str(page_num)+'&status=unrecruited'\n",
    "    req = urlopen(url, context=context)\n",
    "    res = req.read()\n",
    "    soup = BeautifulSoup(res,'html.parser')\n",
    "    for i in range(0, 20):\n",
    "        title = soup.select('.question__title > h3')[i].text.replace('\\n', '').strip()\n",
    "        date = soup.select('.question__info-footer')[i].text.replace('\\n', '')\n",
    "        uploader, date = date.split('·', 1)\n",
    "        date = date.lstrip()\n",
    "        link = soup.select('.question-list-container > ul > li > a')[i]['href']\n",
    "        titles.append(title)\n",
    "        dates.append(date)\n",
    "        links.append('https://www.inflearn.com/'+link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in titles:\n",
    "    if i == '질문드립니다':\n",
    "        num = titles.index(i)\n",
    "        del titles[num]\n",
    "        del links[num]\n",
    "        del dates[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ssl._create_unverified_context()\n",
    "okky_titles = []\n",
    "okky_dates = []\n",
    "okky_links = []\n",
    "# li_tag = []\n",
    "not_good = ['종료', '완료', '마감']\n",
    "page_num = 0\n",
    "# 24 48 72\n",
    "while(page_num<5):\n",
    "    url = 'https://okky.kr/articles/gathering?offset='+str((page_num)*24)+'&max=24&sort=id&order=desc'\n",
    "    req = urlopen(url, context=context)\n",
    "    res = req.read()\n",
    "    soup = BeautifulSoup(res,'html.parser')\n",
    "    for i in range(4, 28):\n",
    "        title = soup.select('.list-title-wrapper.clearfix > h5 > a')[i].text.replace('\\n', '').lstrip().rstrip()\n",
    "        dd = soup.select('div.date-created > span')[i].get_text()\n",
    "        link = soup.select('.list-title-wrapper.clearfix > h5 > a')[i]['href']\n",
    "        if (any(word in title for word in not_good) or len(title) < 3):\n",
    "            continue\n",
    "        else:\n",
    "            okky_titles.append(title)\n",
    "            okky_links.append('https://okky.kr'+link)\n",
    "            day, write_time = dd.split(\" \")\n",
    "            okky_dates.append(day)\n",
    "\n",
    "            # 태그를 추출해 추가하기에는 페이지에서 보여지는 태그가 애매하다.\n",
    "            # 따로 키워드를 선정하거나 할 필요가 있어보임\n",
    "            '''\n",
    "            for j in range(1, 8):\n",
    "                tmp = []\n",
    "                try:\n",
    "                    tag = soup.select('#list-article > div.panel.panel-default.gathering-panel > ul > li:nth-child('+str(i-3)+') > div.list-title-wrapper.clearfix > div > a')[j].get_text()\n",
    "                    tmp.append(tag)\n",
    "                except:\n",
    "                    continue\n",
    "                li_tag.append(tmp)\n",
    "            '''\n",
    "       \n",
    "           \n",
    "    page_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tmp in okky_links:\n",
    "    url = tmp\n",
    "    req = urlopen(url, context=context)\n",
    "    res = req.read()\n",
    "    soup = BeautifulSoup(res,'html.parser')\n",
    "    bad = int(soup.select('.content-eval-count')[0].text)\n",
    "    if bad < 0:\n",
    "        num = okky_links.index(tmp)\n",
    "        del okky_titles[num]\n",
    "        del okky_links[num]\n",
    "        del okky_dates[num]\n",
    "    else: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = []\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    li_tmp = {\"title\": titles[i], \"dday\": dates[i], \"link\": links[i]}\n",
    "    projects.append(li_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(okky_titles)):\n",
    "    li_tmp = {\"title\": okky_titles[i], \"uploaded\": okky_dates[i], \"link\": okky_links[i]}\n",
    "    projects.append(li_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('projects.json', 'w', encoding=\"utf-8\") as make_file: \n",
    "    json.dump(projects, make_file, ensure_ascii = False, indent=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b3d85959e6c4b3d134916c41e259dea5bccfec32db02b044911dc7b04f0df25"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
