{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib.request\n",
    "\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "from dateutil.parser import parse\n",
    "from urllib.request import urlopen\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ssl._create_unverified_context()\n",
    "titles = []\n",
    "dates = []\n",
    "links = []\n",
    "# li_tag = []\n",
    "#main > section.community-body > div.community-body__content > div.question-list-container > ul > li:nth-child(20) > a\n",
    "page_num = 1\n",
    "# 24 48 72\n",
    "for page_num in range(1, 3):\n",
    "    url = 'https://www.inflearn.com/community/studies?page='+str(page_num)+'&status=unrecruited'\n",
    "    req = urlopen(url, context=context)\n",
    "    res = req.read()\n",
    "    soup = BeautifulSoup(res,'html.parser')\n",
    "    for i in range(0, 20):\n",
    "        title = soup.select('.question__title > h3')[i].text.replace('\\n', '').strip()\n",
    "        dday = soup.select('.question__info-footer')[i].text.replace('\\n', '')\n",
    "        dday = dday.split('·', 1)[-1]\n",
    "        dday = dday.lstrip().rstrip()\n",
    "        if '시간' in dday or '분' in dday:\n",
    "            dday = date.today()\n",
    "        else:\n",
    "            day_tmp = dday.split('일')[0]\n",
    "            day_tmp = int(day_tmp)\n",
    "            today = date.today()\n",
    "            day_tmp = datetime.timedelta(days=day_tmp)\n",
    "            dday = today - day_tmp\n",
    "        dday = str(dday)\n",
    "        year, month, day = dday.split('-')\n",
    "        dday = year+'. '+month+'. '+day\n",
    "        link = soup.select('.question-list-container > ul > li > a')[i]['href']\n",
    "        titles.append(title)\n",
    "        dates.append(dday)\n",
    "        links.append('https://www.inflearn.com'+link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in titles:\n",
    "    if i == '질문드립니다':\n",
    "        num = titles.index(i)\n",
    "        del titles[num]\n",
    "        del links[num]\n",
    "        del dates[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ssl._create_unverified_context()\n",
    "okky_titles = []\n",
    "okky_dates = []\n",
    "okky_links = []\n",
    "# li_tag = []\n",
    "not_good = ['종료', '완료', '마감']\n",
    "page_num = 0\n",
    "# 24 48 72\n",
    "while(page_num<5):\n",
    "    url = 'https://okky.kr/articles/gathering?offset='+str((page_num)*24)+'&max=24&sort=id&order=desc'\n",
    "    req = urlopen(url, context=context)\n",
    "    res = req.read()\n",
    "    soup = BeautifulSoup(res,'html.parser')\n",
    "    for i in range(4, 27):\n",
    "        title = soup.select('.list-title-wrapper.clearfix > h5 > a')[i].text.replace('\\n', '').lstrip().rstrip()\n",
    "        dd = soup.select('div.date-created > span')[i].get_text()\n",
    "        link = soup.select('.list-title-wrapper.clearfix > h5 > a')[i]['href']\n",
    "        if (any(word in title for word in not_good) or len(title) < 3):\n",
    "            continue\n",
    "        else:\n",
    "            okky_titles.append(title)\n",
    "            okky_links.append('https://okky.kr'+link)\n",
    "            dday = dd.split(\" \")[0]\n",
    "            year, month, day = dday.split('-')\n",
    "            dday = year+'. '+month+'. '+day\n",
    "            okky_dates.append(dday)\n",
    "\n",
    "            # 태그를 추출해 추가하기에는 페이지에서 보여지는 태그가 애매하다.\n",
    "            # 따로 키워드를 선정하거나 할 필요가 있어보임\n",
    "            '''\n",
    "            for j in range(1, 8):\n",
    "                tmp = []\n",
    "                try:\n",
    "                    tag = soup.select('#list-article > div.panel.panel-default.gathering-panel > ul > li:nth-child('+str(i-3)+') > div.list-title-wrapper.clearfix > div > a')[j].get_text()\n",
    "                    tmp.append(tag)\n",
    "                except:\n",
    "                    continue\n",
    "                li_tag.append(tmp)\n",
    "            '''\n",
    "                \n",
    "    page_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tmp in okky_links:\n",
    "    url = tmp\n",
    "    req = urlopen(url, context=context)\n",
    "    res = req.read()\n",
    "    soup = BeautifulSoup(res,'html.parser')\n",
    "    bad = int(soup.select('.content-eval-count')[0].text)\n",
    "    if bad < 0:\n",
    "        num = okky_links.index(tmp)\n",
    "        del okky_titles[num]\n",
    "        del okky_links[num]\n",
    "        del okky_dates[num]\n",
    "    else: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = []\n",
    "\n",
    "titles = titles + okky_titles\n",
    "dates = dates + okky_dates\n",
    "links = links + okky_links\n",
    "\n",
    "tags = []\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    tag = []\n",
    "    tmp = titles[i]\n",
    "    if 'iOS' in tmp or '앱' in tmp or '게임' in tmp or '소프트웨어' in tmp or '응용' in tmp:        \n",
    "        tag.append('응용')\n",
    "    elif 'AI' in tmp or 'IoT' in tmp or '러닝' in tmp or '인공지능' in tmp:         \n",
    "        tag.append('인공지능')\n",
    "    elif '웹' in tmp or '엔드' in tmp or 'HTML' in tmp or 'web' in tmp:       \n",
    "        tag.append('웹')\n",
    "    elif '데이터' in tmp or 'DB' in tmp or 'Data' in tmp:      \n",
    "        tag.append('데이터')\n",
    "    elif '서버' in tmp or '블록체인' in tmp or '보안' in tmp:\n",
    "        tag.append('서버')\n",
    "    elif 'Unix' in tmp or 'Linux' in tmp or '임베디드' in tmp or '시스템' in tmp:\n",
    "        tag.append('시스템')\n",
    "    else:\n",
    "        tag.append('기타')\n",
    "          \n",
    "    tags.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(okky_titles)):\n",
    "    li_tmp = {\"title\": titles[i], \"uploaded\": dates[i], \"link\": links[i], \"bigtag\":tags[i]}\n",
    "    projects.append(li_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../json 결과/스터디.json', 'w', encoding=\"utf-8\") as make_file: \n",
    "    json.dump(projects, make_file, ensure_ascii = False, indent=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b3d85959e6c4b3d134916c41e259dea5bccfec32db02b044911dc7b04f0df25"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
