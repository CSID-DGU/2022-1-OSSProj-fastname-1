{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incorporated-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "from time import sleep\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "import ssl\n",
    "context=ssl._create_unverified_context()\n",
    "\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aquatic-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "applied-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url_base = 'https://www.thinkcontest.com/Contest/CateField.html?page=1&c=11'\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "res = requests.get(url_base, headers=headers)\n",
    "soup = BeautifulSoup(res.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "key = ['과학/공학', '게임/소프트웨어']\n",
    "links = []\n",
    "titles = []\n",
    "dday = []\n",
    "inst = []\n",
    "dates = []\n",
    "k = 1\n",
    "    \n",
    "while k <= 10:\n",
    "    url = 'https://www.thinkcontest.com/Contest/CateField.html?page=' + str(k) + '&c=11'\n",
    "    base_url = 'https://www.thinkcontest.com/'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "    len_link = len(soup.select(' .txt-left > .contest-title > a'))\n",
    "    for i in range(len_link):\n",
    "        if soup.select(' td > span ')[i].text.replace('\\n', '') == '마감':\n",
    "            break\n",
    "        else:\n",
    "            titles.append(soup.select(' .txt-left > .contest-title > a')[i].text)\n",
    "            links.append(base_url + soup.select('.txt-left > .contest-title > a ')[i]['href'])\n",
    "            dday.append(soup.select(' td > p ')[i].text.split('-')[1])\n",
    "    k=k+1\n",
    "                            \n",
    "str_date = []\n",
    "end_date = []\n",
    "participate = []\n",
    "for i in range(len(links)):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    res = requests.get(links[i], headers=headers)\n",
    "    soup = BeautifulSoup(res.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "    html = soup.select(' tr')\n",
    "    text = str(html).replace('\\n', '')\n",
    "    certi = re.compile('참가자격' + '.{200}')\n",
    "    test = certi.findall(text)[0]\n",
    "    partis = []\n",
    "    if '제한없음' in test:\n",
    "        partis.append('대학(원)생')\n",
    "        pass\n",
    "    elif '일반인' in test:\n",
    "        partis.append('대학(원)생')\n",
    "        pass\n",
    "    elif '국내외 석학과 연구진' in test:\n",
    "        partis.append('대학원생')\n",
    "        pass\n",
    "    elif '대학생' in test:\n",
    "        if '대학원생' in test:\n",
    "            partis.append('대학(원)생')\n",
    "            pass\n",
    "        else :\n",
    "            partis.append('대학생')\n",
    "            pass\n",
    "    elif '대학원생' in test:\n",
    "        partis.append('대학원생')\n",
    "    else : \n",
    "        pass\n",
    "            \n",
    "\n",
    "    participant = str(partis).replace('[', '').replace(']', '').replace(\"'\", \"\")\n",
    "    start = re.compile('접수기간' + '.{19}')\n",
    "    strdate = start.findall(text)[0].split('<td>')[1]\n",
    "    end = re.compile('접수기간' + '.{32}')\n",
    "    enddate = end.findall(text)[0].split('~')[1].replace(' ', '')\n",
    "    participate.append(participant)\n",
    "    str_date.append(strdate)\n",
    "    end_date.append(enddate)\n",
    "    inst.append(soup.select(' tbody > tr > td ')[0].text)\n",
    "        \n",
    "        \n",
    "think_con = []\n",
    "\n",
    "for i in range(len(links)):\n",
    "    li_tmp = {\"title\": titles[i], \"d-day\": end_date[i], \"link\": links[i], \"tag\": participate[i]}\n",
    "    think_con.append(li_tmp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#titles 제목\n",
    "#end_date 마감일\n",
    "#participate 참가자격\n",
    "#links 링크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f81bed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('think_con.json', 'w', encoding=\"utf-8\") as make_file: \n",
    "    json.dump(think_con, make_file, ensure_ascii = False, indent=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86ea388c59ccce0919b9fe36389c4a9e75db8556bbffd122fb12b6b07392e9f4"
  },
  "kernelspec": {
   "display_name": "crawling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
