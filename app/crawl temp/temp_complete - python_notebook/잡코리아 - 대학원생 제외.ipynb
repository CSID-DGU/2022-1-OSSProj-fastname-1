{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f89a2d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b8c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "from urllib.request import urlopen\n",
    "context=ssl._create_unverified_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d17e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "titles = []\n",
    "insts = []\n",
    "end_list = []\n",
    "base_url = 'http://www.jobkorea.co.kr'\n",
    "\n",
    "# 페이지 개수 구해서 넘어가게\n",
    "url= 'https://www.jobkorea.co.kr/starter/?chkSubmit=1&schCareer=&schLocal=&schPart=10016&schMajor=&schEduLevel=5&schWork=&schCType=&isSaved=1&LinkGubun=0&LinkNo=0&Page=1&schType=0&schGid=0&schOrderBy=0&schTxt='\n",
    "headers = {'User-Agent': 'Mozilla/5.0'} \n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "element_num = len(soup.select(' .tit > .link > span'))\n",
    "cnt = int(soup.select(' #TabIngCount')[0].text.replace('(', '').replace(')', '').replace(',', ''))\n",
    "if cnt % element_num == 0:\n",
    "    page_num = cnt / element_num\n",
    "else :\n",
    "    page_num = int(cnt / element_num) + 1\n",
    "    page_num = int(page_num)\n",
    "\n",
    "for k in range(1,page_num+1):\n",
    "    url= 'https://www.jobkorea.co.kr/starter/?chkSubmit=1&schCareer=&schLocal=&schPart=10016&schMajor=&schEduLevel=5&schWork=&schCType=&isSaved=1&LinkGubun=0&LinkNo=0&Page=' + str(k) +'&schType=0&schGid=0&schOrderBy=0&schTxt='\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'} \n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.content.decode('utf-8', 'replace'), 'html.parser')\n",
    "    length = len(soup.select(' div.tit > a '))\n",
    "    for i in range(length):        \n",
    "        titles.append(soup.select(' .tit > .link > span')[i].text)\n",
    "        insts.append(soup.select(' .coTit > .coLink')[i].text)\n",
    "        links.append(base_url + soup.select(' .tit > a')[i+1]['href'])\n",
    "        end_list.append((soup.select(' .side > .day')[i].text).replace(\"~\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4739dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "job_korea = []\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    li_tmp = {\"title\": titles[i], \"d-day\": end_list[i], \"link\": links[i], \"기업\": insts[i]}\n",
    "    job_korea.append(li_tmp)\n",
    "\n",
    "with open('job_korea.json', 'w', encoding=\"utf-8\") as make_file: \n",
    "    json.dump(job_korea, make_file, ensure_ascii = False, indent=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b3d85959e6c4b3d134916c41e259dea5bccfec32db02b044911dc7b04f0df25"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
